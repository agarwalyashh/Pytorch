{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FQtJZqUh4nHP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0PIrPnV6Q-9",
        "outputId": "77a4f964-48c6-49f1-8f71-f0db7e6b0a7e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self,features,labels):\n",
        "    self.features=torch.tensor(features,dtype=torch.float32)\n",
        "    self.labels=torch.tensor(labels,dtype=torch.long)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.features)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    return self.features[idx],self.labels[idx]"
      ],
      "metadata": {
        "id": "QnYbE2gy7jun"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train=pd.read_csv(\"/content/fashion-mnist_train.csv\")\n",
        "test=pd.read_csv(\"/content/fashion-mnist_test.csv\")"
      ],
      "metadata": {
        "id": "YSmGs-lo8IaB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train=train.iloc[:,0].values\n",
        "X_train=train.iloc[:,1:].values\n",
        "y_test=test.iloc[:,0].values\n",
        "X_test=test.iloc[:,1:].values"
      ],
      "metadata": {
        "id": "c5qStUF08rr_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=X_train/255.0\n",
        "X_test=X_test/255.0"
      ],
      "metadata": {
        "id": "3tp2bZmp-JRg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df=CustomDataset(X_train,y_train)"
      ],
      "metadata": {
        "id": "qqEVSp_i9Q89"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZlSHNSq9-v1",
        "outputId": "428c810d-e6d0-4583-8de5-a4556c644a01"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0157, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.2431, 0.2392, 0.0824, 0.1137, 0.0902,\n",
              "         0.2000, 0.5333, 0.2392, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.3451, 0.7882, 0.8941, 0.8824, 1.0000, 0.4510, 0.2431,\n",
              "         0.5373, 1.0000, 0.9216, 0.8706, 1.0000, 0.5294, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.1843, 0.9882, 0.9176, 0.9333, 0.8784, 0.8431, 0.8431, 0.8980,\n",
              "         0.4235, 0.7059, 0.8118, 0.8392, 0.8784, 0.9059, 0.9765, 0.9961, 0.1765,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039,\n",
              "         0.0000, 0.0000, 0.8392, 0.8706, 0.8235, 0.8353, 0.8784, 0.8824, 0.8510,\n",
              "         0.8627, 0.9961, 0.9137, 0.8588, 0.8667, 0.8510, 0.8745, 0.8667, 0.9412,\n",
              "         0.9961, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000,\n",
              "         0.0000, 0.0000, 0.5020, 0.9294, 0.8118, 0.8784, 0.8784, 0.8118, 0.8471,\n",
              "         0.8392, 0.8235, 0.8157, 0.8275, 0.8667, 0.8157, 0.8588, 0.8353, 0.8863,\n",
              "         0.8275, 0.9294, 0.5882, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0078, 0.0000, 0.0000, 0.9294, 0.8706, 0.8431, 0.8118, 0.8235, 0.8314,\n",
              "         0.8353, 0.8078, 0.8392, 0.8353, 0.8392, 0.8353, 0.8235, 0.8431, 0.8392,\n",
              "         0.8078, 0.7804, 0.8549, 1.0000, 0.0510, 0.0000, 0.0078, 0.0000, 0.0000,\n",
              "         0.0000, 0.0157, 0.0000, 0.3333, 0.8941, 0.8235, 0.8549, 0.7843, 0.8275,\n",
              "         0.8157, 0.7961, 0.8431, 0.8235, 0.8196, 0.8196, 0.8235, 0.8353, 0.8275,\n",
              "         0.8235, 0.8510, 0.8078, 0.8353, 0.9059, 0.6863, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.8510, 0.8784, 0.8431, 0.8078, 0.8039,\n",
              "         0.8000, 0.8510, 0.9020, 0.8706, 0.8431, 0.8784, 0.9137, 0.8941, 0.9098,\n",
              "         0.8941, 0.8784, 0.8118, 0.8314, 0.8431, 0.8353, 0.8980, 0.1216, 0.0000,\n",
              "         0.0157, 0.0000, 0.0039, 0.0000, 0.0824, 0.8824, 0.8314, 0.8314, 0.7961,\n",
              "         0.8275, 0.8824, 0.7569, 0.5451, 0.5333, 0.7647, 0.5765, 0.6118, 0.5451,\n",
              "         0.5020, 0.6353, 0.7725, 0.8745, 0.8118, 0.8627, 0.8353, 0.9098, 0.6941,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4824, 0.8863, 0.8118, 0.8275,\n",
              "         0.8196, 0.8039, 0.8941, 0.6196, 0.3529, 0.4039, 0.7294, 0.5412, 0.3922,\n",
              "         0.4745, 0.5765, 0.6196, 0.7176, 0.8863, 0.8157, 0.8392, 0.8196, 0.8471,\n",
              "         1.0000, 0.0510, 0.0000, 0.0039, 0.0000, 0.0000, 0.8863, 0.8588, 0.7922,\n",
              "         0.8157, 0.8078, 0.8039, 0.8471, 0.7216, 0.6118, 0.5882, 0.7569, 0.6667,\n",
              "         0.6431, 0.6588, 0.7373, 0.7294, 0.7843, 0.8588, 0.8471, 0.8353, 0.8353,\n",
              "         0.8275, 0.9137, 0.5804, 0.0000, 0.0000, 0.0000, 0.1765, 0.8902, 0.8000,\n",
              "         0.8392, 0.8275, 0.8549, 0.8706, 0.8667, 0.9020, 0.8980, 0.8667, 0.8353,\n",
              "         0.8784, 0.9137, 0.8863, 0.8627, 0.8588, 0.8667, 0.8784, 0.8745, 0.8510,\n",
              "         0.8235, 0.8549, 0.8353, 0.9961, 0.0000, 0.0000, 0.0000, 0.6157, 0.8863,\n",
              "         0.7961, 0.8118, 0.8275, 0.8196, 0.8431, 0.8039, 0.7765, 0.8118, 0.8157,\n",
              "         0.7882, 0.7882, 0.7725, 0.7961, 0.8039, 0.8235, 0.8118, 0.8353, 0.8392,\n",
              "         0.8392, 0.8392, 0.8353, 0.8157, 0.9176, 0.4196, 0.0000, 0.0000, 0.9216,\n",
              "         0.8353, 0.8000, 0.8275, 0.8235, 0.8196, 0.8353, 0.7922, 0.7725, 0.8000,\n",
              "         0.8431, 0.8510, 0.8353, 0.8314, 0.8235, 0.8078, 0.8314, 0.7961, 0.8275,\n",
              "         0.8549, 0.8431, 0.8392, 0.8157, 0.8196, 0.8706, 0.9020, 0.0000, 0.2039,\n",
              "         1.0000, 0.8118, 0.7843, 0.8157, 0.8353, 0.8235, 0.8235, 0.8157, 0.8118,\n",
              "         0.7922, 0.7882, 0.8196, 0.8471, 0.8471, 0.8471, 0.8471, 0.8392, 0.8314,\n",
              "         0.8039, 0.8431, 0.7882, 0.8941, 0.8157, 0.8392, 0.8314, 0.8549, 0.0980,\n",
              "         0.4627, 0.8510, 0.7882, 0.8078, 0.8157, 0.8353, 0.8157, 0.8039, 0.8078,\n",
              "         0.8235, 0.8275, 0.7922, 0.7804, 0.8118, 0.8157, 0.8196, 0.8235, 0.8118,\n",
              "         0.8235, 0.8235, 0.9608, 0.5451, 0.4667, 1.0000, 0.7922, 0.7961, 0.9255,\n",
              "         0.4471, 0.6706, 0.9333, 0.8314, 0.7961, 0.8627, 0.8471, 0.8510, 0.8196,\n",
              "         0.8118, 0.8039, 0.8235, 0.8275, 0.8078, 0.8000, 0.8078, 0.8196, 0.8275,\n",
              "         0.8431, 0.8235, 0.8078, 0.8667, 0.9490, 0.0000, 0.8784, 0.9176, 0.9020,\n",
              "         0.7098, 0.1020, 0.1529, 0.5686, 0.7882, 1.0000, 0.6157, 0.4510, 0.9804,\n",
              "         0.7843, 0.8118, 0.8078, 0.8118, 0.8353, 0.8471, 0.8078, 0.8039, 0.8078,\n",
              "         0.8118, 0.8078, 0.8431, 0.8118, 0.8667, 0.9333, 0.0000, 0.0000, 0.7373,\n",
              "         0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1216, 0.0000, 0.5059,\n",
              "         0.9922, 0.7451, 0.8118, 0.8157, 0.8157, 0.8157, 0.8196, 0.8275, 0.8275,\n",
              "         0.8196, 0.8196, 0.8196, 0.8314, 0.7882, 0.8863, 0.6471, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.3490, 0.9961, 0.7804, 0.7804, 0.7529, 0.7686, 0.7765, 0.7804, 0.7882,\n",
              "         0.7922, 0.7961, 0.8000, 0.7961, 0.7961, 0.7843, 0.8706, 0.6078, 0.0000,\n",
              "         0.0118, 0.0118, 0.0118, 0.0078, 0.0000, 0.0000, 0.0000, 0.0039, 0.0196,\n",
              "         0.0000, 0.0000, 1.0000, 0.8549, 0.8863, 0.9098, 0.8941, 0.8784, 0.8706,\n",
              "         0.8627, 0.8588, 0.8588, 0.8510, 0.8667, 0.8627, 0.8314, 0.9255, 0.3725,\n",
              "         0.0000, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.6078, 0.7608, 0.6588, 0.6667, 0.6706, 0.6784,\n",
              "         0.6784, 0.7020, 0.6941, 0.6863, 0.6745, 0.6706, 0.6549, 0.6314, 0.7059,\n",
              "         0.0000, 0.0000, 0.0039, 0.0000, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000]),\n",
              " tensor(2))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df=CustomDataset(X_test,y_test)\n",
        "test_df[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxfRwl7n-eEs",
        "outputId": "8ba8dd23-9db6-452b-d7a0-60ee744345fa"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0353, 0.0314,\n",
              "         0.0000, 0.0000, 0.1333, 0.1137, 0.0275, 0.0000, 0.0431, 0.0941, 0.0000,\n",
              "         0.0000, 0.0118, 0.0118, 0.0039, 0.0000, 0.0039, 0.0039, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0157, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.1725, 0.3451, 0.3882, 0.4784, 0.4824, 0.3137,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0078, 0.0000, 0.0000, 0.0000,\n",
              "         0.0118, 0.1804, 0.6824, 0.9765, 0.2627, 0.0000, 0.3686, 0.8235, 0.2392,\n",
              "         0.0549, 0.8314, 0.6157, 0.1451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0078, 0.0078, 0.0000, 0.0902,\n",
              "         0.6588, 0.8078, 0.9490, 0.9373, 0.9333, 0.8392, 0.4902, 0.2392, 0.4431,\n",
              "         0.2902, 0.5216, 0.9255, 0.9333, 0.9255, 0.7961, 0.7216, 0.0784, 0.0000,\n",
              "         0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000,\n",
              "         0.6863, 0.9608, 0.8745, 0.8118, 0.8039, 0.8078, 0.8471, 1.0000, 0.9294,\n",
              "         0.9843, 0.9098, 0.8745, 0.8314, 0.7843, 0.8039, 0.8471, 0.9765, 0.6784,\n",
              "         0.0000, 0.0000, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0275, 0.0000,\n",
              "         0.2078, 0.8824, 0.7882, 0.7725, 0.7843, 0.7882, 0.8078, 0.7804, 0.7725,\n",
              "         0.7255, 0.7608, 0.8000, 0.9098, 0.8863, 0.9765, 0.8588, 0.7608, 0.8039,\n",
              "         0.8980, 0.1294, 0.0000, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039,\n",
              "         0.0000, 0.5216, 0.8745, 0.8157, 0.7529, 0.7647, 0.9137, 0.8863, 0.8471,\n",
              "         0.7490, 0.8235, 0.7373, 0.9255, 0.7294, 0.0000, 0.1961, 0.9176, 0.8118,\n",
              "         0.8157, 0.9059, 0.5216, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.8471, 0.8549, 0.8471, 0.7608, 0.8980, 0.6745, 0.2510,\n",
              "         0.8588, 0.7882, 0.7843, 0.7843, 0.9686, 0.2667, 0.2824, 0.2118, 0.6471,\n",
              "         0.9294, 0.8314, 0.8588, 0.8863, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.1961, 0.8667, 0.8118, 0.8627, 0.8275, 0.8118, 0.6471,\n",
              "         0.5412, 0.8039, 0.7529, 0.7490, 0.7451, 0.9098, 0.4667, 0.4431, 0.2627,\n",
              "         0.6784, 0.9294, 0.8510, 0.8157, 0.8667, 0.1137, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.5137, 0.8471, 0.7843, 0.8588, 0.8118, 0.8314,\n",
              "         0.9059, 0.8863, 0.7569, 0.8392, 0.8784, 0.8078, 0.7961, 0.9020, 0.4784,\n",
              "         0.4392, 0.9176, 0.8784, 0.8392, 0.8000, 0.8784, 0.4824, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.7647, 0.8314, 0.8000, 0.8275, 0.7961,\n",
              "         0.8039, 0.7843, 0.7216, 0.8353, 0.6353, 0.5412, 0.7569, 0.8118, 0.7961,\n",
              "         0.9059, 0.9608, 0.8157, 0.8627, 0.8275, 0.7961, 0.8588, 0.7020, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0314, 0.7255, 0.7490, 0.8549, 0.9137,\n",
              "         0.8588, 0.7882, 0.8667, 0.8353, 0.9647, 0.4471, 0.4980, 0.3137, 0.5059,\n",
              "         0.9098, 0.7765, 0.8549, 0.8118, 0.9255, 0.8902, 0.8627, 0.8471, 0.6745,\n",
              "         0.0824, 0.0000, 0.0000, 0.0000, 0.0000, 0.0824, 0.0157, 0.0196, 0.2510,\n",
              "         0.6275, 0.8784, 0.8784, 0.5647, 0.7333, 0.7725, 0.8275, 0.8118, 0.7294,\n",
              "         0.7529, 0.8235, 0.8314, 0.8549, 0.8824, 0.9255, 0.6941, 0.4157, 0.2196,\n",
              "         0.1098, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0039, 0.0000,\n",
              "         0.0078, 0.0000, 0.4549, 0.9882, 0.3765, 0.4706, 0.2000, 0.2863, 0.2745,\n",
              "         0.4824, 0.3098, 0.2980, 0.2510, 0.6353, 0.9882, 0.4627, 0.0039, 0.0118,\n",
              "         0.0000, 0.0157, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.4510, 0.8863, 0.5686, 0.6667, 0.6078, 0.6471,\n",
              "         0.6314, 0.6235, 0.4902, 0.6863, 0.5490, 0.6824, 0.9255, 0.3725, 0.0000,\n",
              "         0.0078, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0039, 0.0078, 0.0000, 0.5137, 0.8824, 0.8000, 0.8510, 0.8667,\n",
              "         0.8627, 0.8510, 0.8784, 0.9059, 0.8863, 0.9294, 0.7961, 0.9294, 0.4000,\n",
              "         0.0000, 0.0157, 0.0078, 0.0039, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0039, 0.0039, 0.0000, 0.0118, 0.0000, 0.5294, 0.8745, 0.7882, 0.7804,\n",
              "         0.7608, 0.7765, 0.7647, 0.7765, 0.7529, 0.7961, 0.7804, 0.8118, 0.9059,\n",
              "         0.4392, 0.0000, 0.0157, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0039, 0.0039, 0.0000, 0.0039, 0.0000, 0.5255, 0.8745, 0.7804,\n",
              "         0.8078, 0.7804, 0.7882, 0.7843, 0.7961, 0.8078, 0.8118, 0.8235, 0.8078,\n",
              "         0.8902, 0.4667, 0.0000, 0.0118, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.5451, 0.8745,\n",
              "         0.7765, 0.8000, 0.7843, 0.7882, 0.7843, 0.7882, 0.8000, 0.8078, 0.8157,\n",
              "         0.8078, 0.8980, 0.5020, 0.0000, 0.0157, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.5686,\n",
              "         0.8745, 0.7647, 0.8039, 0.7882, 0.7882, 0.7843, 0.8000, 0.8000, 0.8078,\n",
              "         0.8275, 0.8039, 0.9020, 0.5451, 0.0000, 0.0078, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0039, 0.0000, 0.0000,\n",
              "         0.6157, 0.8667, 0.7608, 0.8000, 0.8000, 0.7882, 0.7882, 0.7961, 0.8039,\n",
              "         0.8157, 0.8275, 0.8000, 0.9020, 0.5804, 0.0000, 0.0078, 0.0000, 0.0039,\n",
              "         0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0039, 0.0039, 0.0000,\n",
              "         0.0000, 0.6510, 0.8627, 0.7608, 0.7961, 0.7961, 0.8039, 0.7961, 0.7961,\n",
              "         0.8078, 0.8118, 0.8314, 0.8000, 0.9020, 0.6157, 0.0000, 0.0078, 0.0039,\n",
              "         0.0039, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.6706, 0.8667, 0.7647, 0.8078, 0.7843, 0.7804, 0.7961,\n",
              "         0.7961, 0.8039, 0.8078, 0.8118, 0.8000, 0.8863, 0.7098, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0039, 0.0000, 0.0000, 0.6471, 0.8784, 0.7725, 0.7882, 0.8157, 0.7804,\n",
              "         0.8000, 0.8039, 0.8118, 0.8235, 0.8353, 0.8118, 0.8980, 0.7333, 0.0000,\n",
              "         0.0039, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.5020, 0.7882, 0.7961, 0.7882, 0.8118,\n",
              "         0.8275, 0.7961, 0.8039, 0.8078, 0.8235, 0.8353, 0.8039, 0.8824, 0.7490,\n",
              "         0.0000, 0.0000, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0039, 0.0039, 0.0000, 0.5529, 0.7882, 0.7490, 0.7373,\n",
              "         0.7608, 0.7333, 0.7333, 0.7490, 0.7569, 0.7647, 0.7804, 0.7804, 0.8549,\n",
              "         0.6314, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.8314, 0.9412, 0.8353,\n",
              "         0.9373, 0.9137, 0.9373, 0.9059, 0.9098, 0.9255, 0.9490, 0.9608, 0.8784,\n",
              "         0.9608, 0.9176, 0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1451, 0.2706,\n",
              "         0.3686, 0.4824, 0.4980, 0.5412, 0.5412, 0.5569, 0.5686, 0.5294, 0.4902,\n",
              "         0.4039, 0.3412, 0.2196, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000]),\n",
              " tensor(0))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader=DataLoader(train_df,batch_size=64,shuffle=True,pin_memory=True)\n",
        "test_loader=DataLoader(test_df,batch_size=64,shuffle=False,pin_memory=True)"
      ],
      "metadata": {
        "id": "_do6OK-O-_SS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomNN(nn.Module):\n",
        "  def __init__(self,num_features):\n",
        "    super().__init__()\n",
        "\n",
        "    self.model=nn.Sequential(\n",
        "        nn.Linear(num_features,256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256,64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64,10)\n",
        "        # Need not add softmax explicitely, it is automatically handled\n",
        "    )\n",
        "\n",
        "  def forward(self,X):\n",
        "    return self.model(X)"
      ],
      "metadata": {
        "id": "iE1olTbc_PX1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=50\n",
        "lr=0.1"
      ],
      "metadata": {
        "id": "w9gufzw0__fa"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hA7ErTpsACt3",
        "outputId": "d391b1db-f9b3-46a0-a029-268ce7fa886b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=CustomNN(X_train.shape[1])\n",
        "model=model.to(device)\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.SGD(model.parameters(),lr=lr)"
      ],
      "metadata": {
        "id": "noINPA4-ALAA"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "  total_loss=0\n",
        "  for batch_features,batch_labels in train_loader:\n",
        "    batch_features,batch_labels=batch_features.to(device),batch_labels.to(device)\n",
        "    outputs=model(batch_features)\n",
        "    loss=criterion(outputs,batch_labels)\n",
        "    loss.backward()\n",
        "    optimizer.step() # updates model params\n",
        "    optimizer.zero_grad() # reset gradients to 0\n",
        "    total_loss=total_loss+loss.item()\n",
        "  print(f\"Epoch: {epoch+1}, Loss: {total_loss/len(train_loader)}\")"
      ],
      "metadata": {
        "id": "zeorGTL6Acp0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa3976be-f712-46e5-e5d1-ea6fc52ae3b8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 0.6911799310843574\n",
            "Epoch: 2, Loss: 0.4441897935990586\n",
            "Epoch: 3, Loss: 0.3917527844720304\n",
            "Epoch: 4, Loss: 0.3647293443602921\n",
            "Epoch: 5, Loss: 0.34045874154262706\n",
            "Epoch: 6, Loss: 0.32425739117332103\n",
            "Epoch: 7, Loss: 0.31109482096806007\n",
            "Epoch: 8, Loss: 0.29794868803037\n",
            "Epoch: 9, Loss: 0.28585333578082034\n",
            "Epoch: 10, Loss: 0.2753752179261146\n",
            "Epoch: 11, Loss: 0.2669626146411972\n",
            "Epoch: 12, Loss: 0.2589573455628937\n",
            "Epoch: 13, Loss: 0.25401772401416733\n",
            "Epoch: 14, Loss: 0.24484917496456138\n",
            "Epoch: 15, Loss: 0.23713299310378938\n",
            "Epoch: 16, Loss: 0.23215674214176277\n",
            "Epoch: 17, Loss: 0.22412743175954325\n",
            "Epoch: 18, Loss: 0.22010419183750268\n",
            "Epoch: 19, Loss: 0.21279550645190642\n",
            "Epoch: 20, Loss: 0.20864780854060452\n",
            "Epoch: 21, Loss: 0.20457955574525444\n",
            "Epoch: 22, Loss: 0.19919978669171395\n",
            "Epoch: 23, Loss: 0.19627999318148026\n",
            "Epoch: 24, Loss: 0.1910352736219033\n",
            "Epoch: 25, Loss: 0.1862822117955128\n",
            "Epoch: 26, Loss: 0.1805701792907359\n",
            "Epoch: 27, Loss: 0.176399765845968\n",
            "Epoch: 28, Loss: 0.17251259447740658\n",
            "Epoch: 29, Loss: 0.1707194461795027\n",
            "Epoch: 30, Loss: 0.1661658244016868\n",
            "Epoch: 31, Loss: 0.16160886433086732\n",
            "Epoch: 32, Loss: 0.15759216794676617\n",
            "Epoch: 33, Loss: 0.15472260653885253\n",
            "Epoch: 34, Loss: 0.15281622268473988\n",
            "Epoch: 35, Loss: 0.14797285817730338\n",
            "Epoch: 36, Loss: 0.14653178265711456\n",
            "Epoch: 37, Loss: 0.14128918725171133\n",
            "Epoch: 38, Loss: 0.1396024103608054\n",
            "Epoch: 39, Loss: 0.13566710553300787\n",
            "Epoch: 40, Loss: 0.13600960310334081\n",
            "Epoch: 41, Loss: 0.13349810621536362\n",
            "Epoch: 42, Loss: 0.12714114185096995\n",
            "Epoch: 43, Loss: 0.12618661246725174\n",
            "Epoch: 44, Loss: 0.12208721990079514\n",
            "Epoch: 45, Loss: 0.12014283484884544\n",
            "Epoch: 46, Loss: 0.11808469391136027\n",
            "Epoch: 47, Loss: 0.11489736910826012\n",
            "Epoch: 48, Loss: 0.11118591555964145\n",
            "Epoch: 49, Loss: 0.1115792206953218\n",
            "Epoch: 50, Loss: 0.11831502150148471\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKxdoymsCN30",
        "outputId": "13e622aa-b3fc-4828-8572-c03e39d7e923"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CustomNN(\n",
              "  (model): Sequential(\n",
              "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=64, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total=0\n",
        "correct=0\n",
        "with torch.no_grad():\n",
        "  for batch_features, batch_labels in test_loader:\n",
        "    batch_features, batch_labels=batch_features.to(device), batch_labels.to(device)\n",
        "    outputs=model(batch_features)\n",
        "    _, predicted=torch.max(outputs, 1)\n",
        "    total = total + batch_labels.shape[0]\n",
        "    correct = correct + (predicted == batch_labels).sum().item()\n",
        "print(correct/total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNc4q3flDNDz",
        "outputId": "f26ebfd4-05d5-4bc8-d2a2-87099acf59c9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8984\n"
          ]
        }
      ]
    }
  ]
}